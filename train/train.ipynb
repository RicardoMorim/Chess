{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.pgn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import signal\n",
    "import sys\n",
    "import itertools\n",
    "import csv\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "# Chess Neural Network with adjustable blocks and channels\n",
    "class ChessNet(nn.Module):\n",
    "    def __init__(self, num_blocks=10, channels=256):  # Increased to 10 blocks\n",
    "        super(ChessNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(20, channels, kernel_size=3, padding=1)  # 20 channels for added features\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(channels) for _ in range(num_blocks)])\n",
    "        self.policy_conv = nn.Conv2d(channels, 73, kernel_size=1)\n",
    "        self.policy_bn = nn.BatchNorm2d(73)\n",
    "        self.value_conv = nn.Conv2d(channels, 1, kernel_size=1)\n",
    "        self.value_bn = nn.BatchNorm2d(1)\n",
    "        self.value_fc1 = nn.Linear(64, 256)\n",
    "        self.value_fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        policy = F.relu(self.policy_bn(self.policy_conv(x)))\n",
    "        policy = policy.view(-1, 73 * 8 * 8)\n",
    "        value = F.relu(self.value_bn(self.value_conv(x)))\n",
    "        value = value.view(-1, 64)\n",
    "        value = F.relu(self.value_fc1(value))\n",
    "        value = torch.tanh(self.value_fc2(value))\n",
    "        return policy, value\n",
    "\n",
    "# Updated board_to_tensor with repetition counter and move number\n",
    "def board_to_tensor(board, move_number):\n",
    "    tensor = np.zeros((20, 8, 8), dtype=np.float32)  # Increased to 20 channels\n",
    "    for piece_type in chess.PIECE_TYPES:\n",
    "        for color in chess.COLORS:\n",
    "            for square in board.pieces(piece_type, color):\n",
    "                row, col = divmod(square, 8)\n",
    "                channel = piece_type - 1 if color == chess.WHITE else piece_type + 5\n",
    "                tensor[channel, row, col] = 1\n",
    "    tensor[12, :, :] = board.has_kingside_castling_rights(chess.WHITE)\n",
    "    tensor[13, :, :] = board.has_queenside_castling_rights(chess.WHITE)\n",
    "    tensor[14, :, :] = board.has_kingside_castling_rights(chess.BLACK)\n",
    "    tensor[15, :, :] = board.has_queenside_castling_rights(chess.BLACK)\n",
    "    if board.ep_square is not None:\n",
    "        row, col = divmod(board.ep_square, 8)\n",
    "        tensor[16, row, col] = 1\n",
    "    tensor[17, :, :] = 1 if board.turn == chess.WHITE else 0\n",
    "    tensor[18, :, :] = board.halfmove_clock / 50.0  # Normalized repetition counter (fifty-move rule)\n",
    "    tensor[19, :, :] = move_number / 200.0  # Normalized move number (assuming max 200 moves)\n",
    "    return tensor\n",
    "\n",
    "# Move Index Mapping (unchanged)\n",
    "promotion_moves = {}\n",
    "promotion_idx = 4096\n",
    "for rank in [6, 1]:\n",
    "    for col in range(8):\n",
    "        from_square = chess.square(col, rank)\n",
    "        to_square = chess.square(col, rank + (1 if rank == 6 else -1))\n",
    "        for piece in [chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT]:\n",
    "            promotion_moves[(from_square, to_square, piece)] = promotion_idx\n",
    "            promotion_idx += 1\n",
    "        if col > 0:\n",
    "            to_square = chess.square(col - 1, rank + (1 if rank == 6 else -1))\n",
    "            for piece in [chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT]:\n",
    "                promotion_moves[(from_square, to_square, piece)] = promotion_idx\n",
    "                promotion_idx += 1\n",
    "        if col < 7:\n",
    "            to_square = chess.square(col + 1, rank + (1 if rank == 6 else -1))\n",
    "            for piece in [chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT]:\n",
    "                promotion_moves[(from_square, to_square, piece)] = promotion_idx\n",
    "                promotion_idx += 1\n",
    "\n",
    "def get_move_index(move):\n",
    "    if move.promotion:\n",
    "        return promotion_moves[(move.from_square, move.to_square, move.promotion)]\n",
    "    return move.from_square * 64 + move.to_square\n",
    "\n",
    "# Chess Dataset with Symmetry Augmentation\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, games, augment=True):\n",
    "        self.positions = []\n",
    "        self.augment = augment\n",
    "        for game in games:\n",
    "            result_str = game.headers.get(\"Result\", \"*\")\n",
    "            if result_str not in [\"1-0\", \"0-1\", \"1/2-1/2\"]:\n",
    "                continue\n",
    "            result = {'1-0': 1, '0-1': -1, '1/2-1/2': 0}[result_str]\n",
    "            board = game.board()\n",
    "            move_number = 1\n",
    "            for move in game.mainline_moves():\n",
    "                input_tensor = board_to_tensor(board, move_number)\n",
    "                policy_target = get_move_index(move)\n",
    "                value_target = result if board.turn == chess.WHITE else -result\n",
    "                self.positions.append((input_tensor, policy_target, value_target))\n",
    "                if self.augment:  # Mirror horizontally\n",
    "                    mirrored_board = board.mirror()\n",
    "                    mirrored_move = chess.Move(\n",
    "                        chess.square_mirror(move.from_square),\n",
    "                        chess.square_mirror(move.to_square),\n",
    "                        move.promotion\n",
    "                    )\n",
    "                    mirrored_tensor = board_to_tensor(mirrored_board, move_number)\n",
    "                    mirrored_policy = get_move_index(mirrored_move)\n",
    "                    self.positions.append((mirrored_tensor, mirrored_policy, value_target))\n",
    "                board.push(move)\n",
    "                move_number += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor, policy_target, value_target = self.positions[idx]\n",
    "        return (torch.tensor(input_tensor, dtype=torch.float32),\n",
    "                torch.tensor(policy_target, dtype=torch.long),\n",
    "                torch.tensor(value_target, dtype=torch.float32))\n",
    "\n",
    "# Puzzle Dataset (unchanged for now)\n",
    "class PuzzleDataset(Dataset):\n",
    "    def __init__(self, puzzles):\n",
    "        self.puzzles = puzzles\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.puzzles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fen, move_uci, value_target = self.puzzles[idx]\n",
    "        board = chess.Board(fen)\n",
    "        move = chess.Move.from_uci(move_uci)\n",
    "        input_tensor = board_to_tensor(board, 0)  # Move number not tracked in puzzles\n",
    "        policy_target = get_move_index(move)\n",
    "        return (torch.tensor(input_tensor, dtype=torch.float32),\n",
    "                torch.tensor(policy_target, dtype=torch.long),\n",
    "                torch.tensor(value_target, dtype=torch.float32))\n",
    "\n",
    "# Load puzzles (unchanged)\n",
    "def load_puzzles(pgn_file):\n",
    "    puzzles = []\n",
    "    with open(pgn_file, encoding='ISO-8859-1') as pgn:\n",
    "        while True:\n",
    "            game = chess.pgn.read_game(pgn)\n",
    "            if game is None:\n",
    "                break\n",
    "            board = game.board()\n",
    "            try:\n",
    "                best_move = list(game.mainline_moves())[0]\n",
    "                fen = board.fen()\n",
    "                move_uci = best_move.uci()\n",
    "                puzzles.append((fen, move_uci, 1.0))\n",
    "            except IndexError:\n",
    "                continue\n",
    "    return puzzles\n",
    "\n",
    "def load_lichess_puzzles(csv_file):\n",
    "    puzzles = []\n",
    "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            fen = row['FEN']\n",
    "            moves = row['Moves'].split()\n",
    "            if moves:\n",
    "                move_uci = moves[0]\n",
    "                value_target = 1.0 if 'mate' in row['Themes'].lower() else 0.5\n",
    "                puzzles.append((fen, move_uci, value_target))\n",
    "    return puzzles\n",
    "\n",
    "# Load games in batches (unchanged)\n",
    "def load_games_in_batches(pgn_files, state_file, batch_size=1500):\n",
    "    total_games = 0\n",
    "    processed_games = 0\n",
    "    if os.path.exists(state_file):\n",
    "        with open(state_file, 'r') as f:\n",
    "            state = json.load(f)\n",
    "            processed_games = state.get(\"processed_games\", 0)\n",
    "        print(f\"Resuming from {processed_games} processed games\")\n",
    "    games = []\n",
    "    for file in pgn_files:\n",
    "        with open(file) as pgn:\n",
    "            while True:\n",
    "                game = chess.pgn.read_game(pgn)\n",
    "                if game is None:\n",
    "                    break\n",
    "                total_games += 1\n",
    "                if total_games <= processed_games:\n",
    "                    continue\n",
    "                games.append(game)\n",
    "                if len(games) == batch_size:\n",
    "                    yield games\n",
    "                    processed_games += len(games)\n",
    "                    games = []\n",
    "    if games:\n",
    "        yield games\n",
    "        processed_games += len(games)\n",
    "    with open(state_file, 'w') as f:\n",
    "        json.dump({\"processed_games\": processed_games}, f)\n",
    "\n",
    "# Self-play game generation (basic implementation)\n",
    "def generate_self_play_games(model, num_games=100):\n",
    "    games = []\n",
    "    model.eval()\n",
    "    for _ in range(num_games):\n",
    "        board = chess.Board()\n",
    "        game = chess.pgn.Game()\n",
    "        game.headers[\"Result\"] = \"*\"\n",
    "        node = game\n",
    "        move_number = 1\n",
    "        while not board.is_game_over():\n",
    "            input_tensor = torch.tensor(board_to_tensor(board, move_number), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                policy_logits, _ = model(input_tensor)\n",
    "            policy = F.softmax(policy_logits, dim=1).cpu().numpy()[0]\n",
    "            legal_moves = list(board.legal_moves)\n",
    "            move_probs = np.zeros(len(legal_moves))\n",
    "            for idx, move in enumerate(legal_moves):\n",
    "                move_idx = get_move_index(move)\n",
    "                move_probs[idx] = policy[move_idx]\n",
    "            move_probs /= move_probs.sum()  # Normalize\n",
    "            move = np.random.choice(legal_moves, p=move_probs)\n",
    "            board.push(move)\n",
    "            node = node.add_variation(move)\n",
    "            move_number += 1\n",
    "        result = board.result()\n",
    "        game.headers[\"Result\"] = result\n",
    "        games.append(game)\n",
    "    return games\n",
    "\n",
    "# Updated Training Function with Weighted Loss and Scheduler\n",
    "def train_batch(model, game_dataset, puzzle_dataset, save_path, state_file, epochs=5, processed_games=0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Added L2 regularization\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)  # Cosine annealing scheduler\n",
    "    policy_loss_fn = nn.CrossEntropyLoss()\n",
    "    value_loss_fn = nn.MSELoss()\n",
    "\n",
    "    game_dataloader = DataLoader(game_dataset, batch_size=32, shuffle=True)\n",
    "    puzzle_dataloader = DataLoader(puzzle_dataset, batch_size=32, shuffle=True)\n",
    "    puzzle_iter = itertools.cycle(puzzle_dataloader)\n",
    "\n",
    "    if os.path.exists(state_file):\n",
    "        with open(state_file, 'r') as f:\n",
    "            state = json.load(f)\n",
    "            start_epoch = state.get(\"last_epoch\", 0)\n",
    "            print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "    else:\n",
    "        state = {\"processed_games\": 0, \"last_epoch\": 0}\n",
    "        start_epoch = 0\n",
    "\n",
    "    puzzle_frequency = 1\n",
    "    policy_weight = 1.5  # Emphasize policy early\n",
    "    value_weight = 1.0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + start_epoch):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        game_batch_count = 0\n",
    "        for game_batch in game_dataloader:\n",
    "            inputs, policy_targets, value_targets = game_batch\n",
    "            inputs = inputs.to(device)\n",
    "            policy_targets = policy_targets.to(device)\n",
    "            value_targets = value_targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            policy_logits, value_pred = model(inputs)\n",
    "            policy_loss = policy_loss_fn(policy_logits, policy_targets)\n",
    "            value_loss = value_loss_fn(value_pred.squeeze(), value_targets)\n",
    "            loss = policy_weight * policy_loss + value_weight * value_loss  # Weighted loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            game_batch_count += 1\n",
    "\n",
    "            if game_batch_count % puzzle_frequency == 0:\n",
    "                puzzle_batch = next(puzzle_iter)\n",
    "                inputs, policy_targets, value_targets = puzzle_batch\n",
    "                inputs = inputs.to(device)\n",
    "                policy_targets = policy_targets.to(device)\n",
    "                value_targets = value_targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                policy_logits, value_pred = model(inputs)\n",
    "                policy_loss = policy_loss_fn(policy_logits, policy_targets)\n",
    "                value_loss = value_loss_fn(value_pred.squeeze(), value_targets)\n",
    "                loss = policy_weight * policy_loss + value_weight * value_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()  # Update learning rate\n",
    "        num_puzzle_batches = len(game_dataloader) // puzzle_frequency\n",
    "        avg_loss = total_loss / (len(game_dataloader) + num_puzzle_batches)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        state[\"last_epoch\"] = epoch + 1\n",
    "        state[\"processed_games\"] = processed_games + len(game_dataset)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        with open(state_file, 'w') as f:\n",
    "            json.dump(state, f)\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "# Signal handler (updated)\n",
    "def signal_handler(sig, frame, model, save_path, state_file, processed_games, current_epoch):\n",
    "    print(\"\\nTraining interrupted! Saving model and state...\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    with open(state_file, 'w') as f:\n",
    "        state = {\"processed_games\": processed_games, \"last_epoch\": current_epoch + 1}\n",
    "        json.dump(state, f)\n",
    "    print(f\"Model saved to {save_path}, state saved to {state_file}\")\n",
    "    sys.exit(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = ChessNet(num_blocks=10, channels=256).to(device)  # Adjustable blocks and channels\n",
    "save_path = \"./chess_model/chess_model.pth\"\n",
    "state_file = \"./chess_model/training_state.json\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    print(f\"Loaded existing model from {save_path}\")\n",
    "\n",
    "current_epoch = 0\n",
    "processed_games = 0\n",
    "if os.path.exists(state_file):\n",
    "    with open(state_file, 'r') as f:\n",
    "        state = json.load(f)\n",
    "        processed_games = state.get(\"processed_games\", 0)\n",
    "        current_epoch = state.get(\"last_epoch\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "# Main execution with self-play integration\n",
    "if __name__ == \"__main__\":\n",
    "    signal.signal(signal.SIGINT, lambda sig, frame: signal_handler(sig, frame, model, save_path, state_file, processed_games, current_epoch))\n",
    "    signal.signal(signal.SIGTERM, lambda sig, frame: signal_handler(sig, frame, model, save_path, state_file, processed_games, current_epoch))\n",
    "\n",
    "    pgn_directory = \"./chess_pgns\"\n",
    "    pgn_files = glob.glob(os.path.join(pgn_directory, \"*.pgn\"))\n",
    "\n",
    "    puzzle_pgn = \"./chess_pgns/puzzles/puzzles.pgn\"\n",
    "    pgn_puzzles = load_puzzles(puzzle_pgn)\n",
    "    print(f\"Loaded {len(pgn_puzzles)} PGN puzzles\")\n",
    "\n",
    "    lichess_csv = \"./chess_pgns/puzzles/lichess_db_puzzle.csv\"\n",
    "    lichess_puzzles = load_lichess_puzzles(lichess_csv)\n",
    "    print(f\"Loaded {len(lichess_puzzles)} Lichess puzzles\")\n",
    "\n",
    "    all_puzzles = pgn_puzzles + lichess_puzzles\n",
    "    puzzle_dataset = PuzzleDataset(all_puzzles)\n",
    "    print(f\"Total puzzles: {len(all_puzzles)}\")\n",
    "\n",
    "\n",
    "    # Process games in batches and integrate self-play\n",
    "    for batch_num, games in enumerate(load_games_in_batches(pgn_files, state_file, batch_size=1500), 1):\n",
    "        if not games:\n",
    "            continue\n",
    "        print(f\"\\nProcessing batch {batch_num} with {len(games)} games\")\n",
    "        game_dataset = ChessDataset(games, augment=True)\n",
    "        train_batch(model, game_dataset, puzzle_dataset, save_path, state_file, epochs=5, processed_games=processed_games)\n",
    "        processed_games += len(games)\n",
    "        del game_dataset\n",
    "        gc.collect()\n",
    "\n",
    "        # Generate and train on self-play games every batch\n",
    "        print(\"Generating self-play games...\")\n",
    "        self_play_games = generate_self_play_games(model, num_games=2)  # Small number to start\n",
    "        self_play_dataset = ChessDataset(self_play_games, augment=True)\n",
    "        train_batch(model, self_play_dataset, puzzle_dataset, save_path, state_file, epochs=2, processed_games=processed_games)\n",
    "        del self_play_dataset\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
